"""
News handler

This module provides the NewsHandler class for processing news data.
"""

import time
import re
import json
from tqdm import tqdm
from autoagentsai.client import ChatClient
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.database import NewsDatabase
from config.config_manager import ConfigManager
from scraper.techcrunch_scraper import TechCrunchScraper
from scraper.verge_scraper import VergeScraper
from scraper.github_trending_scraper import GitHubTrendingScraper
from scraper.product_hunt_scraper import ProductHuntScraper
from scraper.a16z_scraper import A16zScraper
from scraper.kr36_scraper import Kr36Scraper


class NewsHandler:
    """Handler for processing news from multiple sources"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        """Initialize news handler"""
        if config_manager is None:
            config_manager = ConfigManager()
        
        self.config = config_manager
        self.database = NewsDatabase()
        
        # Initialize scrapers
        self.scrapers = [
            TechCrunchScraper(),
            VergeScraper(),
            GitHubTrendingScraper(),
            ProductHuntScraper(),
            A16zScraper(),
            Kr36Scraper()
        ]
        
        # Initialize AI client (unified for JSON processing)
        self._ai_client = None
        
    def get_ai_client(self) -> ChatClient:
        """Get unified AI client instance with lazy initialization"""
        if self._ai_client is None:
            try:
                ai_config = self.config.get_ai_agent_config()
                self._ai_client = ChatClient(
                    agent_id=ai_config["agent_id"],
                    personal_auth_key=ai_config["personal_auth_key"],
                    personal_auth_secret=ai_config["personal_auth_secret"]
                )
            except Exception as e:
                print(f"âŒ AI å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ’¡ è¯·æ£€æŸ¥ config.yml ä¸­çš„ ai_agent é…ç½®")
                raise e
        return self._ai_client
    
    @staticmethod
    def get_target_date() -> str:
        """Get target date for news (yesterday)"""
        return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    
    def translate_title(self, title: str) -> str:
        """Translate English title to Chinese using unified AI client"""
        prompt = f"è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–°é—»æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š\n\n{title}"
        
        try:
            client = self.get_ai_client()
            print(f"ğŸŒ ä½¿ç”¨ç»Ÿä¸€AIç¿»è¯‘æ ‡é¢˜")
            content = ""
            for event in client.invoke(prompt):
                if event['type'] == 'token':
                    content += event['content']
            return content.strip()
        except Exception as e:
            print(f"âš ï¸  æ ‡é¢˜ç¿»è¯‘å¤±è´¥: {e}")
            return title  # Return original title if translation fails
    
    def summarize_content(self, content: str) -> str:
        """Summarize news content using unified AI client"""
        prompt = f"è¯·å¯¹ä»¥ä¸‹è‹±æ–‡æ–°é—»å†…å®¹ç”¨ä¸­æ–‡è¿›è¡Œæ€»ç»“ï¼Œæ€»ç»“å†…å®¹ä¸è¶…è¿‡100ä¸ªæ±‰å­—ï¼Œåªè¿”å›æ€»ç»“ç»“æœï¼š\n\n{content}"
        
        try:
            client = self.get_ai_client()
            print(f"ğŸ“ ä½¿ç”¨ç»Ÿä¸€AIæ€»ç»“å†…å®¹")
            summary = ""
            for event in client.invoke(prompt):
                if event['type'] == 'token':
                    summary += event['content']
            return summary.strip()
        except Exception as e:
            print(f"âš ï¸  å†…å®¹æ€»ç»“å¤±è´¥: {e}")
            return "æ–°é—»å†…å®¹æ€»ç»“å¤±è´¥"
    
    def fetch_all_news(self) -> List[Dict[str, Any]]:
        """Fetch news from all configured scrapers (limit 3 per scraper)"""
        all_news = []
        
        for scraper in self.scrapers:
            try:
                scraper_name = scraper.__class__.__name__
                print(f"ğŸ“° å¼€å§‹è·å– {scraper_name} æ–°é—»...")
                news_list = scraper.get_news_list()
                
                # Limit to 3 news items per scraper
                limited_news = news_list[:3]
                all_news.extend(limited_news)
                
                print(f"âœ… {scraper_name} è·å–åˆ° {len(news_list)} ç¯‡æ–°é—»ï¼Œé€‰å–å‰ {len(limited_news)} ç¯‡")
            except Exception as e:
                print(f"âŒ {scraper_name} è·å–æ–°é—»å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ¯ æ€»å…±è·å–åˆ° {len(all_news)} ç¯‡æ–°é—»")
        return all_news
    
    def process_news_item(self, news_item: Dict[str, Any]) -> Dict[str, Any]:
        """Process a single news item with translation and summarization"""
        try:
            title = news_item['title']
            content = news_item['content']
            tag = news_item.get('tag', '')
            
            # Skip title translation for GitHub and Product Hunt (they are usually in English already)
            skip_translation_tags = ['GitHub', 'Product Hunt']
            
            if tag in skip_translation_tags:
                print(f"â­ï¸ è·³è¿‡ {tag} æ ‡é¢˜ç¿»è¯‘ï¼ˆå·²ä¸ºè‹±æ–‡ï¼‰")
                zh_title = title  # Use original title
                
                # Still summarize content with a shorter delay
                print(f"â³ è·³è¿‡ç¿»è¯‘ï¼Œç­‰å¾…5ç§’åè¿›è¡Œæ€»ç»“...")
                time.sleep(5)
            else:
                # Translate title for other sources
                zh_title = self.translate_title(title)
                
                # Add 20 second delay between translation and summarization
                print(f"â³ ç¿»è¯‘å®Œæˆï¼Œç­‰å¾…20ç§’åè¿›è¡Œæ€»ç»“...")
                time.sleep(20)
            
            summary = self.summarize_content(content)
            
            return {
                "date": self.get_target_date(),
                "title": title,
                "zh_title": zh_title,
                "link": news_item['link'],
                "content": content,
                "summary": summary,
                "tag": news_item['tag']
            }
        except Exception as e:
            print(f"âŒ å¤„ç†æ–°é—»é¡¹ç›®å¤±è´¥: {e}")
            return None
    
    def batch_process_news_with_ai(self, news_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process all news items with AI to get JSON format results"""
        print(f"ğŸ¤– æ‰¹é‡å¤„ç† {len(news_items)} ç¯‡æ–°é—»ï¼ˆJSONæ ¼å¼ï¼‰")
        
        try:
            # Prepare news data for AI processing
            news_data = []
            for i, news_item in enumerate(news_items):
                item_data = {
                    "id": i + 1,
                    "source_name": news_item.get('tag', 'Unknown'),
                    "original_title": news_item.get('title', ''),
                    "content": news_item.get('content', '')[:1000]  # Limit content length
                }
                news_data.append(item_data)
            
            # Create AI query for JSON processing
            query = f"""è¯·å¤„ç†ä»¥ä¸‹{len(news_data)}ç¯‡æ–°é—»ï¼Œå¯¹æ¯ç¯‡æ–°é—»è¿›è¡Œç¿»è¯‘å’Œæ€»ç»“ã€‚

è¦æ±‚ï¼š
1. å¯¹äºsource_nameä¸º"GitHub"æˆ–"Product Hunt"çš„æ–°é—»ï¼Œæ ‡é¢˜ä¸éœ€è¦ç¿»è¯‘ï¼Œç›´æ¥ä½¿ç”¨åŸæ ‡é¢˜
2. å¯¹äºå…¶ä»–æ¥æºçš„è‹±æ–‡æ ‡é¢˜ï¼Œç¿»è¯‘æˆä¸­æ–‡
3. å¯¹æ‰€æœ‰å†…å®¹è¿›è¡Œæ€»ç»“ï¼Œæ§åˆ¶åœ¨80å­—å·¦å³ï¼Œä¸å°‘äº60å­—ï¼Œä¸å¤Ÿçš„å°±æ‰©å†™ï¼Œå¤Ÿçš„å°±ç²¾ç®€
4. ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬

è¿”å›æ ¼å¼ï¼ˆJSONæ•°ç»„ï¼‰ï¼š
[
  {{
    "id": 1,
    "source_name": "æ¥æºåç§°",
    "title": "å¤„ç†åçš„æ ‡é¢˜ï¼ˆä¸­æ–‡æˆ–åŸæ–‡ï¼‰",
    "summary": "å†…å®¹æ€»ç»“"
  }},
  ...
]

æ–°é—»æ•°æ®ï¼š
"""
            
            for item in news_data:
                query += f"""
æ–°é—» {item['id']}:
- æ¥æº: {item['source_name']}
- æ ‡é¢˜: {item['original_title']}
- å†…å®¹: {item['content']}
"""
            
            # Send to AI for processing
            print("ğŸ¤– å‘é€AIå¤„ç†è¯·æ±‚...")
            ai_result = self.summarize_content(query)
            
            print(f"ğŸ” AIè¿”å›é•¿åº¦: {len(ai_result)} å­—ç¬¦")
            print(f"ğŸ“‹ AIè¿”å›ç¤ºä¾‹: {ai_result[:200]}...")
            
            # Parse JSON result
            try:
                # Extract JSON from AI response
                
                # Find JSON array in response
                start_idx = ai_result.find('[')
                end_idx = ai_result.rfind(']') + 1
                
                if start_idx == -1 or end_idx == 0:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ•°ç»„")
                
                json_str = ai_result[start_idx:end_idx]
                parsed_results = json.loads(json_str)
                
                print(f"âœ… æˆåŠŸè§£æ {len(parsed_results)} æ¡ç»“æœ")
                
                # Convert to final format
                processed_news = []
                for result in parsed_results:
                    news_id = result.get('id', 0) - 1  # Convert to 0-based index
                    if 0 <= news_id < len(news_items):
                        original_item = news_items[news_id]
                        processed_item = {
                            "date": self.get_target_date(),
                            "title": original_item.get('title', ''),
                            "zh_title": result.get('title', original_item.get('title', '')),
                            "link": original_item.get('link', ''),
                            "content": original_item.get('content', ''),
                            "summary": result.get('summary', 'æ— æ€»ç»“'),
                            "tag": result.get('source_name', original_item.get('tag', ''))
                        }
                        processed_news.append(processed_item)
                
                return processed_news
                
            except (json.JSONDecodeError, ValueError) as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                print(f"ğŸ“„ åŸå§‹è¿”å›: {ai_result}")
                
                # Fallback: create basic results
                return self._create_fallback_results(news_items)
                
        except Exception as e:
            print(f"âŒ AIå¤„ç†å¤±è´¥: {e}")
            return self._create_fallback_results(news_items)
    
    def _create_fallback_results(self, news_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Create fallback results when AI processing fails"""
        print("ğŸ”„ ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆå¤„ç†æ–°é—»")
        processed_news = []
        
        for news_item in news_items:
            processed_item = {
                "date": self.get_target_date(),
                "title": news_item.get('title', ''),
                "zh_title": news_item.get('title', ''),  # Use original title as fallback
                "link": news_item.get('link', ''),
                "content": news_item.get('content', ''),
                "summary": "AIå¤„ç†å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“",
                "tag": news_item.get('tag', '')
            }
            processed_news.append(processed_item)
        
        return processed_news
    


    def process_news(self) -> List[Dict[str, Any]]:
        """Main method to process all news with JSON-based AI processing"""
        target_date = self.get_target_date()
        print(f"ğŸš€ å¼€å§‹å¤„ç† {target_date} çš„æ–°é—»")
        
        # Step 1: Fetch all raw news
        print("ğŸ“¥ é˜¶æ®µ1ï¼šè·å–æ‰€æœ‰æ–°é—»å†…å®¹")
        raw_news = self.fetch_all_news()
        
        if not raw_news:
            print("ğŸ“­ æ²¡æœ‰è·å–åˆ°æ–°é—»æ•°æ®")
            return []
        
        print(f"âœ… æˆåŠŸè·å– {len(raw_news)} æ¡æ–°é—»")
        
        # Step 2: AIæ‰¹é‡å¤„ç†ï¼ˆJSONæ ¼å¼ï¼‰- åŒ…å«ç¿»è¯‘å’Œæ€»ç»“
        print("\nğŸ¤– é˜¶æ®µ2ï¼šAIæ‰¹é‡å¤„ç†ï¼ˆç¿»è¯‘ + æ€»ç»“ï¼‰")
        processed_news = self.batch_process_news_with_ai(raw_news)
        
        if not processed_news:
            print("âŒ AIå¤„ç†å¤±è´¥ï¼Œæ²¡æœ‰è·å¾—æœ‰æ•ˆç»“æœ")
            return []
        
        print(f"âœ… AIå¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(processed_news)} æ¡ç»“æœ")
        
        # Step 3: Save to database
        print("\nğŸ’¾ é˜¶æ®µ3ï¼šä¿å­˜åˆ°æ•°æ®åº“")
        if processed_news:
            try:
                success_count = self.database.insert_news_batch(processed_news)
                print(f"ğŸ’¾ æˆåŠŸä¿å­˜ {success_count}/{len(processed_news)} æ¡æ–°é—»åˆ°æ•°æ®åº“")
            except Exception as e:
                print(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
        
        print(f"âœ… æ–°é—»å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(processed_news)} æ¡æ–°é—»")
        return processed_news


# Legacy functions for backward compatibility
def get_today_date():
    """Legacy function for getting target date"""
    return NewsHandler.get_target_date()

def translate_news_title(title: str) -> str:
    """Legacy function for translating news title"""
    handler = NewsHandler()
    return handler.translate_title(title)

def summarize_news_content(content: str) -> str:
    """Legacy function for summarizing news content"""
    handler = NewsHandler()
    return handler.summarize_content(content)

def news_run():
    """Legacy function for running news processing"""
    handler = NewsHandler()
    return handler.process_news()


if __name__ == "__main__":
    handler = NewsHandler()
    news_list = handler.process_news()
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼Œå…± {len(news_list)} æ¡æ–°é—»")
