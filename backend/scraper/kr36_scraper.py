"""
36kr news scraper

This module provides the Kr36Scraper class for scraping tech news from 36kr.
"""

import requests
from tqdm import tqdm
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict, Any


class Kr36Scraper:
    """36kr scraper for Chinese tech and startup news"""
    
    BASE_URL = "https://36kr.com"
    AI_URL = "https://36kr.com/search/articles/AI"
    SOURCE_TAG = "36kr"
    
    def __init__(self):
        """Initialize the 36kr scraper"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://36kr.com/',
            'Connection': 'keep-alive',
        }
    
    @staticmethod
    def get_today_date() -> str:
        """Get yesterday's date in YYYY-MM-DD format"""
        return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    
    @staticmethod
    def get_recent_dates(days: int = 3) -> List[str]:
        """Get list of recent dates for news filtering"""
        today = datetime.now()
        return [
            (today - timedelta(days=i)).strftime("%Y-%m-%d") 
            for i in range(days)
        ]
    
    def get_title_and_link_list(self) -> List[Dict[str, Any]]:
        """Scrape AI and tech news from 36kr"""
        news_list = []
        target_dates = self.get_recent_dates()
        print(f"ğŸ” 36kr: æŸ¥æ‰¾æ—¥æœŸ {target_dates}")
        
        # Try multiple URLs
        urls_to_try = [
            "https://36kr.com/",  # Main page
            "https://36kr.com/search/articles/AI",  # AI search
            "https://36kr.com/search/articles/äººå·¥æ™ºèƒ½",  # AI search in Chinese
        ]
        
        for url in urls_to_try:
            try:
                print(f"ğŸ“„ æ­£åœ¨æŠ“å–é¡µé¢: {url}")
                response = requests.get(url, headers=self.headers, timeout=30)
                
                if response.status_code == 403:
                    print(f"âš ï¸  36kr è®¿é—®è¢«é™åˆ¶: {url}")
                    continue
                
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'lxml')
                
                # Look for article links
                articles = soup.find_all('a', href=True)
                articles = [a for a in articles if '/p/' in a.get('href', '') or '/news/' in a.get('href', '')]
                
                print(f"ğŸ“‹ æ‰¾åˆ° {len(articles)} ä¸ªæ½œåœ¨æ–‡ç« é“¾æ¥")
                
                found_articles = 0
                for article in articles:
                    try:
                        href = article.get('href')
                        title = article.text.strip()
                        
                        if not href or not title or len(title) < 5:
                            continue
                        
                        # Skip unwanted links
                        if any(skip in href for skip in ['#', 'mailto:', 'tel:', 'javascript:', '/author/', '/tag/']):
                            continue
                        
                        # Filter for AI/tech content
                        ai_keywords = ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ç§‘æŠ€', 'åˆ›ä¸š', 'äº’è”ç½‘', 'æ•°æ®', 'ç®—æ³•', 'è‡ªåŠ¨åŒ–', 'æ•°å­—åŒ–', 'æ™ºèƒ½', 'æŠ€æœ¯']
                        if not any(keyword in title.lower() for keyword in ai_keywords):
                            continue
                        
                        # Build full URL
                        if href.startswith('/'):
                            link = f'{self.BASE_URL}{href}'
                        elif href.startswith('http'):
                            link = href
                        else:
                            continue
                        
                        # Extract date from URL if possible
                        try:
                            import re
                            # 36kr URLs often have patterns like /p/2445678 or timestamp
                            # For now, use current date as we can't easily extract date
                            article_date = self.get_today_date()
                        except:
                            article_date = self.get_today_date()
                        
                        # Add to news list (skip date filtering for Chinese sites as they're less predictable)
                        print(f"âœ… æ‰¾åˆ°æ–‡ç« : {title[:30]}...")
                        news_list.append({
                            'title': title,
                            'link': link,
                            'date': article_date,
                            'tag': self.SOURCE_TAG
                        })
                        found_articles += 1
                        
                        # Limit to prevent too many requests
                        if found_articles >= 8:
                            break
                            
                    except Exception as e:
                        print(f"âŒ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ğŸ“Š ä» {url} æ‰¾åˆ° {found_articles} ç¯‡ç§‘æŠ€æ–‡ç« ")
                
                if found_articles > 0:
                    break  # If we found articles, don't try other URLs
                    
            except Exception as e:
                print(f"âŒ æŠ“å–36kré¡µé¢æ—¶å‡ºé”™: {url}, é”™è¯¯: {e}")
                continue
        
        # Remove duplicates
        seen_links = set()
        unique_news = []
        for news in news_list:
            if news['link'] not in seen_links:
                seen_links.add(news['link'])
                unique_news.append(news)
        
        print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(unique_news)} ç¯‡ 36kr æ–‡ç« ")
        return unique_news
    
    def get_news_content(self, news_link: str) -> str:
        """Extract content from a 36kr article"""
        try:
            response = requests.get(news_link, headers=self.headers, timeout=30)
            
            if response.status_code == 403:
                return "36krå†…å®¹è®¿é—®å—é™"
            
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Try multiple content selectors for 36kr
            content_div = soup.find('div', class_='kr-rich-text-wrapper') or \
                         soup.find('div', class_='article-content') or \
                         soup.find('div', class_='content') or \
                         soup.find('article') or \
                         soup.find('main')
            
            if content_div:
                paragraphs = content_div.find_all('p')
            else:
                # Fallback to all paragraphs
                paragraphs = soup.find_all('p')
            
            contents = []
            for paragraph in paragraphs:
                text = paragraph.text.strip()
                # Filter out ads, subscribe messages, etc.
                if text and len(text) > 10 and not any(skip in text for skip in ['ä¸‹è½½36æ°ª', 'è®¢é˜…', 'å¹¿å‘Š', 'æ¨å¹¿']):
                    contents.append(text)
            
            # Limit content length for processing
            full_content = "\n".join(contents)
            if len(full_content) > 2000:
                full_content = full_content[:2000] + "..."
            
            return full_content if full_content else "æ— æ³•è·å–æ–‡ç« å†…å®¹"
            
        except Exception as e:
            print(f"âŒ è·å–æ–‡ç« å†…å®¹å¤±è´¥: {news_link}, é”™è¯¯: {e}")
            return "æ— æ³•è·å–æ–‡ç« å†…å®¹"
    
    def get_news_list(self) -> List[Dict[str, Any]]:
        """Get complete news list with content"""
        news_list = self.get_title_and_link_list()
        news_content_list = []
        
        for news in tqdm(news_list, desc="ğŸ¢ Getting 36kr content"):
            news_content = self.get_news_content(news['link'])
            news['content'] = news_content
            news_content_list.append(news)
        
        return news_content_list


def get_news_list():
    """Legacy function for backward compatibility"""
    scraper = Kr36Scraper()
    return scraper.get_news_list()


if __name__ == '__main__':
    scraper = Kr36Scraper()
    news = scraper.get_news_list()
    print(f"âœ… è·å–åˆ° {len(news)} ç¯‡æ–‡ç« ")
    for item in news:
        print(f"  - {item['title']}")
