#!/usr/bin/env python3
"""
Product Hunt çˆ¬è™«

ä½¿ç”¨ Playwright ç»•è¿‡ Cloudflareï¼Œå¹¶æ”¯æŒ domloaded ç­‰å¾…ç­–ç•¥
"""

import os
import sys
import time
import random
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, Page
from tqdm import tqdm
import requests


class ProductHuntScraper:
    """Product Hunt çˆ¬è™«ç±»ï¼Œä½¿ç”¨ Playwright ç»•è¿‡ Cloudflare"""
    
    BASE_URL = "https://www.producthunt.com"
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
    def get_current_week_number(self) -> int:
        """è·å–å½“å‰å‘¨æ•°"""
        return datetime.now().isocalendar()[1]
    
    def get_title_and_link_list_with_playwright(self) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨ Playwright è·å– Product Hunt äº§å“åˆ—è¡¨ï¼Œç»•è¿‡ Cloudflare
        
        Returns:
            List[Dict[str, str]]: åŒ…å«äº§å“ä¿¡æ¯çš„åˆ—è¡¨
        """
        print("ğŸ­ ä½¿ç”¨ Playwright è·å– Product Hunt æ•°æ®...")
        
        # è®¡ç®—ç›®æ ‡URL - ä¼˜å…ˆä½¿ç”¨ä¸Šå‘¨æ•°æ®ï¼ˆå› ä¸ºå½“å‘¨æ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼‰
        current_year = datetime.now().year
        current_week = self.get_current_week_number()
        
        # å°è¯•å¤šä¸ªå¯èƒ½æœ‰æ•°æ®çš„å‘¨ï¼ˆä»æœ€è¿‘å¾€å‰æ‰¾ï¼‰
        urls_to_try = []
        
        # ä»ä¸Šå‘¨å¼€å§‹å¾€å‰æ¨ï¼ˆé¿å…å½“å‰å‘¨å¯èƒ½æ²¡æœ‰æ•°æ®çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å‘¨ä¸€å‡Œæ™¨ï¼‰
        for weeks_back in range(1, 6):  # ä»1å¼€å§‹ï¼Œé¿å…å½“å‰å‘¨
            target_week = current_week - weeks_back
            target_year = current_year
            
            # å¤„ç†è·¨å¹´æƒ…å†µ
            if target_week <= 0:
                target_week += 52
                target_year -= 1
            
            # ç¡®ä¿å‘¨æ•°åœ¨åˆç†èŒƒå›´å†…
            if target_week > 0 and target_week <= 52:
                urls_to_try.append(f"{self.BASE_URL}/leaderboard/weekly/{target_year}/{target_week}")
        
        print(f"ğŸ—“ï¸ å°†å°è¯•è·å–æœ€è¿‘5å‘¨çš„æ•°æ®ï¼ˆè·³è¿‡å½“å‰å‘¨ï¼Œå› ä¸ºå¯èƒ½è¿˜æ²¡æœ‰æ•°æ®ï¼‰...")
        
        # ç¬¬ä¸€ä¸ª URL å°è¯•ä¸¤æ¬¡
        first_url = urls_to_try[0]
        for attempt in range(2):
            print(f"ğŸŒ å°è¯•è®¿é—® (ç¬¬{attempt + 1}æ¬¡): {first_url}")
            try:
                products = self._scrape_with_playwright(first_url)
                if products:
                    print(f"âœ… æˆåŠŸè·å– {len(products)} ä¸ªäº§å“")
                    return products
                else:
                    print(f"âš ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•æœªè·å–åˆ°äº§å“æ•°æ®")
            except Exception as e:
                print(f"âŒ ç¬¬{attempt + 1}æ¬¡è®¿é—®å¤±è´¥: {e}")
                if attempt == 0:  # ç¬¬ä¸€æ¬¡å¤±è´¥åç­‰å¾…ä¸€ä¸‹å†é‡è¯•
                    print("â³ ç­‰å¾… 3 ç§’åé‡è¯•...")
                    time.sleep(3)
        
        # å¦‚æœç¬¬ä¸€ä¸ª URL ä¸¤æ¬¡éƒ½å¤±è´¥ï¼Œå°è¯•ç¬¬äºŒä¸ª URL
        if len(urls_to_try) > 1:
            second_url = urls_to_try[1]
            print(f"ğŸŒ å°è¯•è®¿é—®å¤‡é€‰URL: {second_url}")
            try:
                products = self._scrape_with_playwright(second_url)
                if products:
                    print(f"âœ… æˆåŠŸè·å– {len(products)} ä¸ªäº§å“")
                    return products
                else:
                    print(f"âš ï¸ å¤‡é€‰URLæœªè·å–åˆ°äº§å“æ•°æ®")
            except Exception as e:
                print(f"âŒ å¤‡é€‰URLè®¿é—®å¤±è´¥: {e}")
        
        print("âŒ æ‰€æœ‰URLéƒ½è®¿é—®å¤±è´¥")
        return []
    
    def _scrape_with_playwright(self, url: str) -> List[Dict[str, str]]:
        """
        ä½¿ç”¨ Playwright çˆ¬å–æŒ‡å®šURLçš„äº§å“æ•°æ®
        
        Args:
            url: è¦çˆ¬å–çš„URL
            
        Returns:
            List[Dict[str, str]]: äº§å“åˆ—è¡¨
        """
        try:
            with sync_playwright() as p:
                # å¯åŠ¨æµè§ˆå™¨ï¼Œé…ç½®åæ£€æµ‹å‚æ•°
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-accelerated-2d-canvas',
                        '--no-first-run',
                        '--no-zygote',
                        '--disable-gpu',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--disable-background-timer-throttling',
                        '--disable-backgrounding-occluded-windows',
                        '--disable-renderer-backgrounding',
                    ]
                )
                
                # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    locale='en-US',
                    timezone_id='America/New_York',
                    extra_http_headers={
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                )
                
                page = context.new_page()
                
                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                page.add_init_script("""
                    // ç§»é™¤ webdriver æ ‡è¯†
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined,
                    });
                    
                    // æ¨¡æ‹ŸçœŸå®çš„ navigator å±æ€§
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['en-US', 'en'],
                    });
                    
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5],
                    });
                    
                    // ç§»é™¤è‡ªåŠ¨åŒ–æ ‡è¯†
                    delete window.chrome.loadTimes;
                    delete window.chrome.csi;
                    delete window.chrome.app;
                """)
                
                print("ğŸŒ æ­£åœ¨è®¿é—®é¡µé¢...")
                
                # è®¿é—®é¡µé¢ï¼Œä½¿ç”¨ domcontentloaded ç­‰å¾…ç­–ç•¥
                page.goto(url, wait_until='domcontentloaded', timeout=60000)
                
                print("â³ ç­‰å¾…é¡µé¢åŠ è½½å’Œ Cloudflare éªŒè¯...")
                
                # ç­‰å¾…åˆå§‹åŠ è½½
                page.wait_for_timeout(random.randint(3000, 6000))
                
                # æ£€æŸ¥æ˜¯å¦é‡åˆ° Cloudflare
                initial_content = page.content()
                if any(cf_indicator in initial_content for cf_indicator in [
                    "Just a moment", "Checking your browser", "cf-browser-verification", 
                    "DDoS protection", "cloudflare"
                ]):
                    print("ğŸ”„ æ£€æµ‹åˆ° Cloudflareï¼Œç­‰å¾…éªŒè¯å®Œæˆ...")
                    
                    # æ›´é•¿æ—¶é—´çš„ç­‰å¾…
                    page.wait_for_timeout(random.randint(10000, 15000))
                    
                    # å°è¯•é‡æ–°åŠ è½½é¡µé¢
                    try:
                        page.reload(wait_until='domcontentloaded', timeout=30000)
                        page.wait_for_timeout(random.randint(3000, 5000))
                    except Exception as e:
                        print(f"âš ï¸ é‡æ–°åŠ è½½å¤±è´¥ï¼Œç»§ç»­å°è¯•: {e}")
                
                # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
                print("ğŸ“œ æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹...")
                for i in range(3):
                    page.evaluate(f"window.scrollTo(0, document.body.scrollHeight/3 * {i + 1});")
                    page.wait_for_timeout(random.randint(1000, 2000))
                
                # ç­‰å¾…é¡µé¢ç¨³å®š
                page.wait_for_timeout(3000)
                
                # è·å–æœ€ç»ˆé¡µé¢å†…å®¹ï¼Œå¤„ç†å¯¼èˆªé—®é¢˜
                max_content_attempts = 3
                content = None
                
                for content_attempt in range(max_content_attempts):
                    try:
                        content = page.content()
                        print(f"ğŸ“„ é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                        break
                    except Exception as content_error:
                        if "navigating" in str(content_error).lower():
                            print(f"âš ï¸ é¡µé¢æ­£åœ¨å¯¼èˆªä¸­ï¼ŒåšæŒå½“å‰URLï¼Œç­‰å¾…åé‡è¯• ({content_attempt + 1}/{max_content_attempts})")
                            page.wait_for_timeout(random.randint(3000, 5000))
                            continue
                        else:
                            raise content_error
                
                browser.close()
                
                if not content:
                    print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œé¡µé¢å¯èƒ½åœ¨æŒç»­å¯¼èˆª")
                    return []
                
                # è§£æé¡µé¢å†…å®¹
                return self._parse_product_hunt_content(content, url)
                
        except Exception as e:
            print(f"âŒ Playwright çˆ¬å–å¤±è´¥: {e}")
            raise e
    
    def _parse_product_hunt_content(self, content: str, base_url: str) -> List[Dict[str, str]]:
        """
        è§£æ Product Hunt é¡µé¢å†…å®¹ï¼Œæå–äº§å“ä¿¡æ¯
        
        Args:
            content: é¡µé¢HTMLå†…å®¹
            base_url: åŸºç¡€URL
            
        Returns:
            List[Dict[str, str]]: äº§å“åˆ—è¡¨
        """
        soup = BeautifulSoup(content, 'lxml')
        products = []
        
        print("ğŸ” è§£æé¡µé¢å†…å®¹...")
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰"No posts for this date"çš„æç¤º
        text_content = soup.get_text()
        if "No posts for this date" in text_content:
            print("âš ï¸ è¯¥å‘¨æ²¡æœ‰äº§å“æ•°æ®ï¼Œé¡µé¢æ˜¾ç¤º: 'No posts for this date'")
            return []
        
        # ä¸“é—¨æŸ¥æ‰¾Product Huntæ’è¡Œæ¦œäº§å“ - ä½¿ç”¨å‘ç°çš„æ­£ç¡®é€‰æ‹©å™¨
        product_selectors = [
            # çœŸæ­£çš„äº§å“å…ƒç´ é€‰æ‹©å™¨ï¼ˆåŸºäºç”¨æˆ·å‘ç°ï¼‰
            '[data-test^="post-item-"]',  # ç²¾ç¡®åŒ¹é… post-item-æ•°å­— æ ¼å¼
            'section[data-test^="post-item-"]',  # section æ ‡ç­¾çš„äº§å“é¡¹
            # å¤‡é€‰é€‰æ‹©å™¨
            '[data-test*="post-item"]',
            '[data-test*="product"]',
            '[data-testid*="product"]', 
            '.product-item',
            '.leaderboard-item',
            '[class*="product"]',
            '[class*="leaderboard"]',
            # é€šç”¨çš„æ’è¡Œæ¦œé¡¹ç›®é€‰æ‹©å™¨
            '[data-test*="item"]',
            '.item',
            'li',
            'article'
        ]
        
        product_elements = []
        for selector in product_selectors:
            elements = soup.select(selector)
            if elements:
                print(f"ğŸ¯ ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                product_elements.extend(elements)
                if len(product_elements) >= 20:  # æ‰¾åˆ°è¶³å¤Ÿå¤šçš„å…ƒç´ å°±åœæ­¢
                    break
        
        if not product_elements:
            print("ğŸ” æœªæ‰¾åˆ°äº§å“å®¹å™¨ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾äº§å“é“¾æ¥...")
            # æŸ¥æ‰¾äº§å“é“¾æ¥ - Product Hunt çš„äº§å“é€šå¸¸ä½¿ç”¨ /posts/ è·¯å¾„
            all_links = soup.find_all('a', href=True)
            post_links = []
            
            for link in all_links:
                href = link.get('href', '')
                text = link.get_text().strip()
                
                # ä¸¥æ ¼ç­›é€‰çœŸæ­£çš„äº§å“é“¾æ¥
                if (href and 
                    '/posts/' in href and 
                    text and 
                    len(text) > 2 and 
                    len(text) < 100 and
                    not any(skip in text.lower() for skip in [
                        'login', 'signup', 'about', 'terms', 'privacy', 'help',
                        'see more', 'view all', 'launch', 'coming soon', 'archive'
                    ])):
                    post_links.append(link)
            
            print(f"ğŸ¯ æ‰¾åˆ° {len(post_links)} ä¸ªäº§å“é“¾æ¥")
            
            if post_links:
                product_elements = post_links
            else:
                print("âŒ æœªæ‰¾åˆ°ä»»ä½•äº§å“å…ƒç´ ")
                return []
        
        # å¤„ç†æ‰¾åˆ°çš„äº§å“å…ƒç´ 
        seen_urls = set()
        seen_titles = set()
        
        for element in product_elements:
            try:
                # ä»å…ƒç´ ä¸­æå–äº§å“ä¿¡æ¯
                product_info = self._extract_product_from_element(element, base_url)
                
                if product_info and product_info['link'] not in seen_urls:
                    title_key = product_info['title'].lower().strip()
                    
                    # è¿‡æ»¤æ˜æ˜¾ä¸æ˜¯äº§å“çš„é¡¹ç›®
                    if (len(product_info['title']) > 2 and 
                        title_key not in seen_titles and
                        not any(skip in title_key for skip in [
                            'see more', 'view all', 'launch', 'coming soon', 'archive',
                            'newsletter', 'category', 'leaderboard', 'forum'
                        ])):
                        
                        seen_urls.add(product_info['link'])
                        seen_titles.add(title_key)
                        products.append(product_info)
                        
                        # åªå–å‰5ä¸ªäº§å“
                        if len(products) >= 5:
                            break
                            
            except Exception as e:
                print(f"âš ï¸ å¤„ç†äº§å“å…ƒç´ æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"âœ… æˆåŠŸè§£æ {len(products)} ä¸ªäº§å“")
        return products
    
    def _extract_product_from_element(self, element, base_url: str) -> Optional[Dict[str, str]]:
        """
        ä»HTMLå…ƒç´ ä¸­æå–äº§å“ä¿¡æ¯
        
        Args:
            element: HTMLå…ƒç´ 
            base_url: åŸºç¡€URL
            
        Returns:
            Dict[str, str]: äº§å“ä¿¡æ¯ï¼Œå¦‚æœæå–å¤±è´¥åˆ™è¿”å›None
        """
        try:
            # æŸ¥æ‰¾äº§å“é“¾æ¥ï¼Œä¼˜å…ˆæŸ¥æ‰¾ /posts/ æ ¼å¼çš„é“¾æ¥
            link_element = None
            
            # æ–¹æ³•1: æŸ¥æ‰¾æŒ‡å‘ /posts/ çš„é“¾æ¥ï¼ˆè¿™æ˜¯çœŸæ­£çš„äº§å“é“¾æ¥ï¼‰
            post_links = element.find_all('a', href=lambda x: x and '/posts/' in x)
            if post_links:
                link_element = post_links[0]  # å–ç¬¬ä¸€ä¸ªäº§å“é“¾æ¥
            
            # æ–¹æ³•2: å¦‚æœå…ƒç´ æœ¬èº«å°±æ˜¯é“¾æ¥
            elif element.name == 'a':
                link_element = element
                
            # æ–¹æ³•3: åœ¨å…ƒç´ å†…æŸ¥æ‰¾ä»»ä½•é“¾æ¥
            else:
                link_element = element.find('a', href=True)
            
            if not link_element:
                return None
            
            href = link_element.get('href', '')
            if not href:
                return None
                
            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯äº§å“çš„é“¾æ¥
            if any(skip in href.lower() for skip in [
                'header_nav', 'footer', 'sponsor', 'newsletter', 'category', 
                'login', 'signup', 'help', 'about', 'terms', 'privacy'
            ]):
                return None
            
            # æ„å»ºå®Œæ•´URL
            if href.startswith('/'):
                full_url = urljoin(self.BASE_URL, href)
            else:
                full_url = href
            
            # æå–æ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨aria-labelè·å–å¹²å‡€çš„äº§å“åç§°
            title = ""
            
            # æ–¹æ³•1: ä¼˜å…ˆä»é“¾æ¥çš„aria-labelè·å–äº§å“åç§°ï¼ˆæœ€å¹²å‡€çš„æ–¹å¼ï¼‰
            if link_element.get('aria-label'):
                aria_label = link_element.get('aria-label').strip()
                if aria_label:
                    # æ¸…ç†aria-labelï¼Œæå–äº§å“åç§°
                    title = aria_label
                    # ç§»é™¤å¸¸è§çš„å‰ç¼€/åç¼€
                    title = title.replace(' on Product Hunt', '')
                    title = title.replace(' - Product Hunt', '')
                    title = title.replace('View ', '')
                    title = title.replace('See ', '')
                    title = title.replace('Visit ', '')
                    title = title.strip()
            
            # æ–¹æ³•2: ä»post-itemå…ƒç´ ä¸­æŸ¥æ‰¾å…¶ä»–aria-label
            if not title and 'post-item' in str(element.get('data-test', '')):
                # æŸ¥æ‰¾å­å…ƒç´ çš„aria-label
                for child in element.find_all(attrs={'aria-label': True}):
                    aria_label = child.get('aria-label', '').strip()
                    if aria_label and len(aria_label) > 2 and len(aria_label) < 50:
                        title = aria_label
                        # æ¸…ç†æ ‡é¢˜
                        title = title.replace(' on Product Hunt', '')
                        title = title.replace(' - Product Hunt', '')
                        title = title.strip()
                        break
            
            # æ–¹æ³•3: ä»é“¾æ¥æ–‡æœ¬è·å–ï¼ˆå»æ‰å¤šä½™ä¿¡æ¯ï¼‰
            if not title or len(title) < 3:
                link_text = link_element.get_text().strip()
                if link_text:
                    # åªå–å‰å‡ ä¸ªè¯ä½œä¸ºäº§å“åç§°
                    words = link_text.split()
                    if words:
                        # å–å‰1-3ä¸ªæœ‰æ„ä¹‰çš„è¯
                        title = ' '.join(words[:3])
            
            # æ–¹æ³•4: ä»é“¾æ¥çš„titleå±æ€§è·å–
            if not title or len(title) < 3:
                title = link_element.get('title', '') or link_element.get('alt', '')
                if title:
                    title = title.replace(' on Product Hunt', '').strip()
            
            # æ¸…ç†æ ‡é¢˜
            if title:
                title = ' '.join(title.split())
                # ç§»é™¤å¸¸è§çš„åç¼€
                suffixes_to_remove = [
                    " - Product Hunt", " | Product Hunt", " on Product Hunt",
                    " - PH", " | PH", " (PH)"
                ]
                for suffix in suffixes_to_remove:
                    if title.endswith(suffix):
                        title = title[:-len(suffix)]
            
            if not title or len(title) < 2:
                return None
            
            # æå–æ—¥æœŸ
            date = self._extract_date_from_url(base_url)
            
            return {
                'title': title,
                'link': full_url,
                'date': date,
                'tag': "Product Hunt"
            }
            
        except Exception as e:
            print(f"âš ï¸ æå–äº§å“ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _extract_date_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–æ—¥æœŸä¿¡æ¯"""
        try:
            # URLæ ¼å¼: /leaderboard/weekly/2024/52
            parts = url.split('/')
            if len(parts) >= 6 and 'weekly' in parts:
                year = int(parts[-2])
                week = int(parts[-1])
                
                # è®¡ç®—è¯¥å‘¨çš„å¼€å§‹æ—¥æœŸ
                jan_1 = datetime(year, 1, 1)
                week_start = jan_1 + timedelta(weeks=week-1)
                week_start = week_start - timedelta(days=week_start.weekday())
                
                return week_start.strftime('%Y-%m-%d')
        except:
            pass
        
        # é»˜è®¤è¿”å›å½“å‰æ—¥æœŸ
        return datetime.now().strftime('%Y-%m-%d')
    
    def get_title_and_link_list(self) -> List[Dict[str, str]]:
        """
        è·å–äº§å“æ ‡é¢˜å’Œé“¾æ¥åˆ—è¡¨ï¼ˆä¸»æ–¹æ³•ï¼‰
        
        Returns:
            List[Dict[str, str]]: äº§å“åˆ—è¡¨
        """
        try:
            # ä¼˜å…ˆä½¿ç”¨ Playwright æ–¹æ³•
            return self.get_title_and_link_list_with_playwright()
        except Exception as e:
            print(f"âŒ Playwright æ–¹æ³•å¤±è´¥: {e}")
            return []
    
    def get_news_content(self, url: str) -> str:
        """
        è·å–å•ä¸ªäº§å“çš„è¯¦ç»†å†…å®¹ï¼ˆè¿›å…¥äº§å“é¡µé¢è·å–çœŸå®æ ‡é¢˜å’Œæè¿°ï¼‰
        
        Args:
            url: äº§å“é¡µé¢URL
            
        Returns:
            str: äº§å“æè¿°å†…å®¹
        """
        print(f"ğŸ“– è·å–äº§å“è¯¦ç»†ä¿¡æ¯: {url}")
        
        # å®ç°é‡è¯•æœºåˆ¶ï¼Œç±»ä¼¼æ’è¡Œæ¦œé¡µé¢
        max_attempts = 5
        
        for attempt in range(max_attempts):
            print(f"ğŸ”„ äº§å“é¡µé¢å°è¯• {attempt + 1}/{max_attempts}")
            
            try:
                result = self._get_product_content_single_attempt(url)
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°çœŸå®å†…å®¹ï¼ˆä¸æ˜¯Cloudflareé¡µé¢ï¼‰
                if (result and 
                    "Verifying you are human" not in result and 
                    "Just a moment" not in result and
                    "www.producthunt.com" not in result):
                    return result
                else:
                    print(f"âš ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•è·å–åˆ°çš„å¯èƒ½æ˜¯Cloudflareé¡µé¢")
                    if attempt < max_attempts - 1:
                        print("â³ ç­‰å¾…3ç§’åé‡è¯•...")
                        time.sleep(3)
                        
            except Exception as e:
                print(f"âŒ ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥: {e}")
                if attempt < max_attempts - 1:
                    print("â³ ç­‰å¾…3ç§’åé‡è¯•...")
                    time.sleep(3)
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä»URLæå–çš„äº§å“å
        product_slug = url.split('/')[-1].replace('-', ' ').title()
        print(f"âš ï¸ æ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨URLä¸­çš„äº§å“å: {product_slug}")
        return f"{product_slug} - Product Hunt äº§å“"
    
    def _get_product_content_single_attempt(self, url: str) -> str:
        """
        å•æ¬¡å°è¯•è·å–äº§å“å†…å®¹
        
        Args:
            url: äº§å“é¡µé¢URL
            
        Returns:
            str: äº§å“æè¿°å†…å®¹
        """
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-accelerated-2d-canvas',
                        '--no-first-run',
                        '--no-zygote',
                        '--disable-gpu',
                        '--disable-blink-features=AutomationControlled',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--disable-background-timer-throttling',
                        '--disable-backgrounding-occluded-windows',
                        '--disable-renderer-backgrounding',
                    ]
                )
                
                context = browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    locale='en-US',
                    timezone_id='America/New_York',
                    extra_http_headers={
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                    }
                )
                
                page = context.new_page()
                
                # åæ£€æµ‹è„šæœ¬
                page.add_init_script("""
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined,
                    });
                    
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['en-US', 'en'],
                    });
                    
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5],
                    });
                    
                    delete window.chrome.loadTimes;
                    delete window.chrome.csi;
                    delete window.chrome.app;
                """)
                
                print("ğŸŒ è®¿é—®äº§å“é¡µé¢...")
                
                # è®¿é—®äº§å“é¡µé¢
                page.goto(url, wait_until='domcontentloaded', timeout=60000)
                page.wait_for_timeout(random.randint(3000, 6000))
                
                print("âœ… è®¿é—®äº§å“é¡µé¢")
                
                # æ»šåŠ¨é¡µé¢ä»¥ç¡®ä¿å†…å®¹åŠ è½½ï¼ˆæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼‰
                for i in range(5):
                    try:
                        page.evaluate(f"""
                            if (document.body && document.body.scrollHeight) {{
                                window.scrollTo(0, document.body.scrollHeight/2 * {i + 1});
                            }} else {{
                                window.scrollTo(0, window.innerHeight * {i + 1});
                            }}
                        """)
                        page.wait_for_timeout(random.randint(1000, 2000))
                    except Exception as scroll_error:
                        print(f"âš ï¸ æ»šåŠ¨å¤±è´¥ï¼Œè·³è¿‡: {scroll_error}")
                        break
                
                page.wait_for_timeout(3000)
                
                # è·å–é¡µé¢å†…å®¹ï¼Œå¤„ç†å¯¼èˆªé—®é¢˜
                max_content_attempts = 3
                content = None
                
                for content_attempt in range(max_content_attempts):
                    try:
                        content = page.content()
                        break
                    except Exception as content_error:
                        if "navigating" in str(content_error).lower():
                            print(f"âš ï¸ äº§å“é¡µé¢æ­£åœ¨å¯¼èˆªä¸­ï¼ŒåšæŒå½“å‰URLï¼Œç­‰å¾…åé‡è¯• ({content_attempt + 1}/{max_content_attempts})")
                            page.wait_for_timeout(random.randint(3000, 5000))
                            continue
                        else:
                            raise content_error
                
                browser.close()
                
                if not content:
                    print("âŒ æ— æ³•è·å–äº§å“é¡µé¢å†…å®¹ï¼Œé¡µé¢å¯èƒ½åœ¨æŒç»­å¯¼èˆª")
                    return "æ— æ³•è·å–äº§å“è¯¦æƒ…"
                
                # è§£æäº§å“é¡µé¢å†…å®¹
                soup = BeautifulSoup(content, 'lxml')
                
                # æå–äº§å“åç§°
                product_name = ""
                
                # å°è¯•å¤šç§æ–¹å¼æå–äº§å“åç§°
                name_selectors = [
                    'h1',  # ä¸»æ ‡é¢˜
                    '[data-test*="product-name"]',
                    '[data-test*="title"]',
                    '.product-title',
                    '.product-name',
                    'meta[property="og:title"]',
                    'title'
                ]
                
                for selector in name_selectors:
                    elements = soup.select(selector)
                    for element in elements:
                        if selector.startswith('meta'):
                            text = element.get('content', '')
                        else:
                            text = element.get_text().strip()
                        
                        if text and len(text) > 2 and len(text) < 100:
                            # æ¸…ç†æ ‡é¢˜
                            text = ' '.join(text.split())
                            # ç§»é™¤ Product Hunt ç›¸å…³åç¼€
                            suffixes_to_remove = [
                                " - Product Hunt", " | Product Hunt", " on Product Hunt",
                                " - PH", " | PH", " (PH)"
                            ]
                            for suffix in suffixes_to_remove:
                                if text.endswith(suffix):
                                    text = text[:-len(suffix)]
                            
                            product_name = text
                            break
                    
                    if product_name:
                        break
                
                # æå–äº§å“æè¿°
                product_description = ""
                
                # å°è¯•å¤šç§æ–¹å¼æå–äº§å“æè¿°
                description_selectors = [
                    'meta[property="og:description"]',
                    'meta[name="description"]',
                    '[data-test*="product-description"]',
                    '[data-test*="description"]',
                    '.product-description',
                    '.description',
                    '.tagline',
                    'p'  # æœ€åå°è¯•ç¬¬ä¸€ä¸ªæ®µè½
                ]
                
                for selector in description_selectors:
                    elements = soup.select(selector)
                    for element in elements:
                        if selector.startswith('meta'):
                            text = element.get('content', '')
                        else:
                            text = element.get_text().strip()
                        
                        # é€‰æ‹©åˆé€‚é•¿åº¦çš„æè¿°
                        if text and len(text) > 20 and len(text) < 1000:
                            # æ¸…ç†æè¿°
                            text = ' '.join(text.split())
                            
                            # ç§»é™¤ Product Hunt ç›¸å…³åç¼€
                            suffixes_to_remove = [
                                " - Product Hunt", " | Product Hunt", " on Product Hunt"
                            ]
                            for suffix in suffixes_to_remove:
                                if text.endswith(suffix):
                                    text = text[:-len(suffix)]
                            
                            product_description = text
                            break
                    
                    if product_description:
                        break
                
                # ç»„åˆäº§å“åç§°å’Œæè¿°
                if product_name and product_description:
                    # ç¡®ä¿æè¿°ä¸é‡å¤äº§å“åç§°
                    if not product_description.lower().startswith(product_name.lower()):
                        final_content = f"{product_name} - {product_description}"
                    else:
                        final_content = product_description
                    
                    print(f"âœ… æå–åˆ°: {product_name}")
                    print(f"ğŸ“ æè¿°: {product_description[:100]}...")
                    return final_content
                    
                elif product_name:
                    print(f"âœ… ä»…æå–åˆ°äº§å“åç§°: {product_name}")
                    return product_name
                    
                elif product_description:
                    print(f"âœ… ä»…æå–åˆ°äº§å“æè¿°: {product_description[:100]}...")
                    return product_description
                
                # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å› URL ä¸­çš„äº§å“å
                product_slug = url.split('/')[-1].replace('-', ' ').title()
                print(f"âš ï¸ ä½¿ç”¨URLä¸­çš„äº§å“å: {product_slug}")
                return f"{product_slug} - Product Hunt äº§å“"
                
        except Exception as e:
            print(f"âŒ è·å–äº§å“è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
            # è¿”å›ä»URLæå–çš„äº§å“åä½œä¸ºå¤‡é€‰
            product_slug = url.split('/')[-1].replace('-', ' ').title()
            return f"{product_slug} - Product Hunt äº§å“"
    
    def get_news_list(self) -> List[Dict[str, str]]:
        """
        è·å–å®Œæ•´çš„æ–°é—»åˆ—è¡¨ï¼ˆåŒ…å«å†…å®¹ï¼‰
        
        Returns:
            List[Dict[str, str]]: åŒ…å«å®Œæ•´ä¿¡æ¯çš„äº§å“åˆ—è¡¨
        """
        print("ğŸ“° è·å–å®Œæ•´çš„ Product Hunt äº§å“åˆ—è¡¨...")
        
        # å…ˆè·å–æ ‡é¢˜å’Œé“¾æ¥åˆ—è¡¨
        products = self.get_title_and_link_list()
        
        if not products:
            print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•äº§å“")
            return []
        
        print(f"ğŸ“ ä¸º {len(products)} ä¸ªäº§å“è·å–è¯¦ç»†å†…å®¹...")
        
        # ä¸ºæ¯ä¸ªäº§å“è·å–è¯¦ç»†å†…å®¹ï¼Œæ·»åŠ  tqdm è¿›åº¦æ¡
        complete_products = []
        for i, product in enumerate(tqdm(products, desc="ğŸ† Getting Product Hunt content")):
            try:
                print(f"ğŸ“– ({i+1}/{len(products)}) è·å–: {product['title']}")
                
                # è·å–å†…å®¹
                content = self.get_news_content(product['link'])
                product['content'] = content
                
                complete_products.append(product)
                
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(products) - 1:  # æœ€åä¸€ä¸ªä¸éœ€è¦å»¶è¿Ÿ
                    delay = random.randint(1, 3)
                    time.sleep(delay)
                    
            except Exception as e:
                print(f"âŒ è·å–äº§å“ {product['title']} çš„å†…å®¹å¤±è´¥: {e}")
                # å³ä½¿è·å–å†…å®¹å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸºæœ¬ä¿¡æ¯
                product['content'] = "æ— æ³•è·å–äº§å“æè¿°"
                complete_products.append(product)
        
        print(f"âœ… æˆåŠŸè·å– {len(complete_products)} ä¸ªå®Œæ•´äº§å“ä¿¡æ¯")
        return complete_products


    def get_top_products_of_week(self, limit: int = 5) -> List[Dict[str, str]]:
        """
        è·å–æœ¬å‘¨æ’åå‰Nçš„äº§å“
        
        Args:
            limit: è¿”å›çš„äº§å“æ•°é‡é™åˆ¶
            
        Returns:
            List[Dict[str, str]]: æœ¬å‘¨æ’åå‰Nçš„äº§å“åˆ—è¡¨
        """
        print(f"ğŸ† è·å–æœ¬å‘¨æ’åå‰ {limit} çš„äº§å“...")
        
        products = self.get_title_and_link_list()
        
        if not products:
            print("âŒ æœªè·å–åˆ°ä»»ä½•äº§å“")
            return []
        
        # é™åˆ¶è¿”å›æ•°é‡
        top_products = products[:limit]
        
        print(f"âœ… æˆåŠŸè·å–æœ¬å‘¨æ’åå‰ {len(top_products)} çš„äº§å“")
        return top_products


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç  - è·å–æœ¬å‘¨æ’åå‰5çš„äº§å“
    scraper = ProductHuntScraper()
    top_products = scraper.get_top_products_of_week(5)
    
    if top_products:
        print(f"\nğŸ† æœ¬å‘¨Product Huntæ’åå‰ {len(top_products)} çš„äº§å“:")
        print("=" * 60)
        
        for i, product in enumerate(top_products):
            print(f"ç¬¬ {i+1} å: {product['title']}")
            print(f"   ğŸ”— é“¾æ¥: {product['link']}")
            print(f"   ğŸ“… æ—¥æœŸ: {product['date']}")
            print(f"   ğŸ·ï¸ æ¥æº: {product['tag']}")
            print()
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•äº§å“")
