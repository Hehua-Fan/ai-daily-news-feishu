"""
Product Hunt news scraper

This module provides the ProductHuntScraper class for scraping trending products from Product Hunt.
"""

import requests
from tqdm import tqdm
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from typing import List, Dict, Any


class ProductHuntScraper:
    """Product Hunt scraper for trending AI and tech products"""
    
    BASE_URL = "https://www.producthunt.com"
    SOURCE_TAG = "ProductHunt"
    
    def __init__(self):
        """Initialize the Product Hunt scraper"""
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Referer': 'https://www.producthunt.com/',
        }
    
    @staticmethod
    def get_today_date() -> str:
        """Get yesterday's date in YYYY-MM-DD format"""
        return (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
    
    @staticmethod
    def get_recent_dates(days: int = 3) -> List[str]:
        """Get list of recent dates for product filtering"""
        today = datetime.now()
        return [
            (today - timedelta(days=i)).strftime("%Y-%m-%d") 
            for i in range(days)
        ]
    
    def get_title_and_link_list(self) -> List[Dict[str, Any]]:
        """Scrape trending products from Product Hunt (weekly)"""
        news_list = []
        target_dates = self.get_recent_dates(7)  # Check last 7 days for weekly
        print(f"ğŸ” Product Hunt: æŸ¥æ‰¾æ—¥æœŸ {target_dates}")
        
        # Try weekly leaderboard first
        try:
            # Get current week's start date (Monday)
            today = datetime.now()
            days_since_monday = today.weekday()
            monday = today - timedelta(days=days_since_monday)
            week_str = monday.strftime("%Y/%m/%d")
            
            url = f'{self.BASE_URL}/leaderboard/weekly/{week_str}'
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–æœ¬å‘¨æ’è¡Œæ¦œ: {url}")
            
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Look for product cards
            products = soup.find_all('div', {'data-test': 'post-item'})
            if not products:
                # Fallback selector
                products = soup.find_all('li', class_='styles_item__1t9xF')
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(products)} ä¸ªäº§å“")
            
            found_products = 0
            for product in products:
                try:
                    # Find product title link
                    title_link = product.find('a', href=True)
                    if not title_link:
                        continue
                    
                    title = title_link.text.strip()
                    href = title_link.get('href')
                    
                    if not title or not href:
                        continue
                    
                    # Build full URL
                    if href.startswith('/'):
                        link = f'{self.BASE_URL}{href}'
                    else:
                        link = href
                    
                    # Filter AI/tech related products (simple keyword filter)
                    ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'automation', 'tech', 'saas', 'productivity', 'analytics', 'data', 'software', 'app', 'tool', 'platform']
                    if any(keyword in title.lower() for keyword in ai_keywords):
                        formatted_date = self.get_today_date()
                        print(f"âœ… æ‰¾åˆ°AI/ç§‘æŠ€äº§å“: {title[:50]}...")
                        news_list.append({
                            'title': title,
                            'link': link,
                            'date': formatted_date,
                            'tag': self.SOURCE_TAG
                        })
                        found_products += 1
                        
                        # Limit to 10 products to avoid too many
                        if found_products >= 10:
                            break
                        
                except Exception as e:
                    print(f"âŒ å¤„ç†äº§å“æ—¶å‡ºé”™: {e}")
                    continue
            
            print(f"ğŸ“Š æœ¬å‘¨æ’è¡Œæ¦œæ‰¾åˆ° {found_products} ä¸ªAI/ç§‘æŠ€äº§å“")
                
        except Exception as e:
            print(f"âŒ æŠ“å–æœ¬å‘¨æ’è¡Œæ¦œæ—¶å‡ºé”™: {e}")
            
            # Fallback to daily if weekly fails
            print("ğŸ”„ å›é€€åˆ°æ—¥æ’è¡Œæ¦œ...")
            today = datetime.now()
            date_str = today.strftime("%Y/%m/%d")
            formatted_date = today.strftime("%Y-%m-%d")
            
            try:
                url = f'{self.BASE_URL}/leaderboard/daily/{date_str}'
                print(f"ğŸ“„ æ­£åœ¨æŠ“å–ä»Šæ—¥æ’è¡Œæ¦œ: {url}")
                
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'lxml')
                products = soup.find_all('div', {'data-test': 'post-item'})
                
                found_products = 0
                for product in products[:5]:  # Limit to top 5 from daily
                    try:
                        title_link = product.find('a', href=True)
                        if not title_link:
                            continue
                        
                        title = title_link.text.strip()
                        href = title_link.get('href')
                        
                        if not title or not href:
                            continue
                        
                        if href.startswith('/'):
                            link = f'{self.BASE_URL}{href}'
                        else:
                            link = href
                        
                        ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 'automation', 'tech', 'saas', 'productivity', 'analytics', 'data']
                        if any(keyword in title.lower() for keyword in ai_keywords):
                            print(f"âœ… æ‰¾åˆ°AI/ç§‘æŠ€äº§å“: {title[:50]}...")
                            news_list.append({
                                'title': title,
                                'link': link,
                                'date': formatted_date,
                                'tag': self.SOURCE_TAG
                            })
                            found_products += 1
                            
                    except Exception as e:
                        print(f"âŒ å¤„ç†äº§å“æ—¶å‡ºé”™: {e}")
                        continue
                
                print(f"ğŸ“Š ä»Šæ—¥æ’è¡Œæ¦œæ‰¾åˆ° {found_products} ä¸ªAI/ç§‘æŠ€äº§å“")
                
            except Exception as e:
                print(f"âŒ æŠ“å–ä»Šæ—¥æ’è¡Œæ¦œä¹Ÿå¤±è´¥: {e}")
        
        print(f"ğŸ¯ æ€»å…±æ‰¾åˆ° {len(news_list)} ä¸ª Product Hunt äº§å“")
        return news_list
    
    def get_news_content(self, news_link: str) -> str:
        """Extract content from a product page"""
        try:
            response = requests.get(news_link, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'lxml')
            
            # Try to find product description
            description_elem = soup.find('div', {'data-test': 'post-description'})
            if not description_elem:
                # Fallback selectors
                description_elem = soup.find('div', class_='styles_htmlText__3lSgj') or \
                                  soup.find('p', class_='styles_postTagline__3o6_Z')
            
            if description_elem:
                description = description_elem.text.strip()
            else:
                description = "äº§å“æè¿°æš‚æ— "
            
            # Try to get maker info
            maker_info = soup.find('div', {'data-test': 'makers-list'})
            maker_text = ""
            if maker_info:
                makers = maker_info.find_all('a')
                if makers:
                    maker_names = [maker.text.strip() for maker in makers]
                    maker_text = f"å¼€å‘è€…: {', '.join(maker_names)}"
            
            # Combine information
            content_parts = [description]
            if maker_text:
                content_parts.append(maker_text)
            
            return "\n".join(content_parts)
            
        except Exception as e:
            print(f"âŒ è·å–äº§å“å†…å®¹å¤±è´¥: {news_link}, é”™è¯¯: {e}")
            return "æ— æ³•è·å–äº§å“æè¿°"
    
    def get_news_list(self) -> List[Dict[str, Any]]:
        """Get complete product list with content"""
        news_list = self.get_title_and_link_list()
        news_content_list = []
        
        for news in tqdm(news_list, desc="ğŸš€ Getting Product Hunt content"):
            news_content = self.get_news_content(news['link'])
            news['content'] = news_content
            news_content_list.append(news)
        
        return news_content_list


def get_news_list():
    """Legacy function for backward compatibility"""
    scraper = ProductHuntScraper()
    return scraper.get_news_list()


if __name__ == '__main__':
    scraper = ProductHuntScraper()
    news = scraper.get_news_list()
    print(f"âœ… è·å–åˆ° {len(news)} ä¸ªäº§å“")
    for item in news:
        print(f"  - {item['title']}")
