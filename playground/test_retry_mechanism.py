#!/usr/bin/env python3
"""
æµ‹è¯•Product Huntäº§å“é‡è¯•æœºåˆ¶
"""

import sys
import os

# Add backend directory to path
backend_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend')
sys.path.append(backend_path)

from scraper.product_hunt_scraper import ProductHuntScraper


def test_retry_mechanism():
    """æµ‹è¯•é‡è¯•æœºåˆ¶çš„æ•ˆæœ"""
    print("ğŸ”„ æµ‹è¯•Product Huntäº§å“é‡è¯•æœºåˆ¶...")
    
    scraper = ProductHuntScraper()
    
    # å…ˆè·å–äº§å“åˆ—è¡¨
    print("ğŸ“‹ è·å–äº§å“åˆ—è¡¨...")
    products = scraper.get_title_and_link_list()
    
    if not products:
        print("âŒ æ²¡æœ‰è·å–åˆ°äº§å“åˆ—è¡¨")
        return
    
    print(f"âœ… è·å–åˆ° {len(products)} ä¸ªäº§å“")
    
    # æµ‹è¯•å‰2ä¸ªäº§å“çš„é‡è¯•æœºåˆ¶
    print(f"\nğŸ”„ æµ‹è¯•å‰2ä¸ªäº§å“çš„é‡è¯•æœºåˆ¶ï¼š")
    
    for i, product in enumerate(products[:2]):
        print(f"\n{'='*60}")
        print(f"äº§å“ {i+1}: {product['title']}")
        print(f"é“¾æ¥: {product['link']}")
        print(f"{'='*60}")
        
        # è·å–è¯¦ç»†å†…å®¹ï¼ˆç°åœ¨æœ‰é‡è¯•æœºåˆ¶ï¼‰
        detailed_content = scraper.get_news_content(product['link'])
        
        print(f"æœ€ç»ˆç»“æœ: {detailed_content}")
        
        # åˆ†æç»“æœ
        if ("Verifying you are human" in detailed_content or 
            "Just a moment" in detailed_content or
            "www.producthunt.com" in detailed_content):
            print("âŒ ä»ç„¶æ˜¯Cloudflareé¡µé¢")
        elif "Product Hunt äº§å“" in detailed_content:
            print("âš ï¸ ä½¿ç”¨äº†å¤‡é€‰æ–¹æ¡ˆï¼ˆURLæå–ï¼‰")
        else:
            print("âœ… æˆåŠŸè·å–åˆ°çœŸå®äº§å“ä¿¡æ¯ï¼")


if __name__ == '__main__':
    test_retry_mechanism()
